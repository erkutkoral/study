{"cells":[{"source":" **The time.time() function shows the time (in seconds) since a pre-defined time, which in Unix-based systems is January 1, 1970.**","metadata":{},"cell_type":"markdown","id":"f9c773ab-e67e-4432-ad3a-f46e9f5e489e"},{"source":"## Measuring time I\nIn the lecture slides, you saw how the time.time() function can be loaded and used to assess the time required to perform a basic mathematical operation.\n\nNow, you will use the same strategy to assess two different methods for solving a similar problem: calculate the sum of squares of all the positive integers from 1 to 1 million (1,000,000).\n\nSimilar to what you saw in the video, you will compare two methods; one that uses brute force and one more mathematically sophisticated.","metadata":{},"cell_type":"markdown","id":"be353f32-cc4a-445c-a804-231603de6d3f"},{"source":"# Calculate the result of the problem using formula() and print the time required\nN = 1000000\nfm_start_time = time.time()\nfirst_method = formula(N)\nprint(\"Time using formula: {} sec\".format(time.time() - fm_start_time))\n\n# Calculate the result of the problem using brute_force() and print the time required\nsm_start_time = time.time()\nsecond_method = brute_force(N)\nprint(\"Time using the brute force: {} sec\".format(time.time() - sm_start_time))","metadata":{},"cell_type":"code","id":"ca8e533e-079f-4293-93a7-cb15556c4056","execution_count":null,"outputs":[]},{"source":"## Measuring time II\nAs we discussed in the lectures, in the majority of cases, a list comprehension is faster than a for loop.\n\nIn this demonstration, you will see a case where a list comprehension and a for loop have so small difference in efficiency that choosing either method will perform this simple task instantly.\n\nIn the list words, there are random words downloaded from the Internet. We are interested to create another list called listlet in which we only keep the words that start with the letter b.\n\nIn case you are not familiar with dealing with strings in Python, each string has the .startswith() attribute, which returns a True/False statement whether the string starts with a specific letter/phrase or not.","metadata":{},"cell_type":"markdown","id":"70179706-7f0b-46a4-b0b6-69f4bdab7a6b"},{"source":"import pandas as pd\nimport time\n# Store the time before the execution\nstart_time = time.time()\n\n# Execute the operation\nletlist = [wrd for wrd in words if wrd.startswith('b')]\n\n# Store and print the difference between the start and the current time\ntotal_time_lc = time.time() - start_time\nprint('Time using list comprehension: {} sec'.format(total_time_lc))\n\n# Store the time before the execution\nstart_time = time.time()\n\n# Execute the operation\nletlist = []\nfor wrd in words:\n    if wrd.startswith('b'):\n        letlist.append(wrd)\n        \n# Print the difference between the start and the current time\ntotal_time_fl = time.time() - start_time\nprint('Time using for loop: {} sec'.format(total_time_fl))","metadata":{"executionTime":57,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport time\n# Store the time before the execution\nstart_time = time.time()\n\n# Execute the operation\nletlist = [wrd for wrd in words if wrd.startswith('b')]\n\n# Store and print the difference between the start and the current time\ntotal_time_lc = time.time() - start_time\nprint('Time using list comprehension: {} sec'.format(total_time_lc))\n\n# Store the time before the execution\nstart_time = time.time()\n\n# Execute the operation\nletlist = []\nfor wrd in words:\n    if wrd.startswith('b'):\n        letlist.append(wrd)\n        \n# Print the difference between the start and the current time\ntotal_time_fl = time.time() - start_time\nprint('Time using for loop: {} sec'.format(total_time_fl))","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"6b3d7f35-395e-4512-8385-872203b2217a","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Execute the operation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m letlist \u001b[38;5;241m=\u001b[39m [wrd \u001b[38;5;28;01mfor\u001b[39;00m wrd \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwords\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m wrd\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Store and print the difference between the start and the current time\u001b[39;00m\n\u001b[1;32m     10\u001b[0m total_time_lc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n","\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"],"ename":"NameError","evalue":"name 'words' is not defined"}]},{"source":"## Row selection: loc[] vs iloc[]\nA big part of working with DataFrames is to locate specific entries in the dataset. You can locate rows in two ways:\n\n- By a specific value of a column (feature).\n- By the index of the rows (index). In this exercise, we will focus on the second way.\nIf you have previous experience with pandas, you should be familiar with the .loc and .iloc indexers, which stands for 'location' and 'index location' respectively. In most cases, the indices will be the same as the position of each row in the Dataframe (e.g. the row with index 13 will be the 14th entry).\n\nWhile we can use both functions to perform the same task, we are interested in which is the most efficient in terms of speed.","metadata":{},"cell_type":"markdown","id":"45945f60-d1b2-490a-81eb-c4841a55d628"},{"source":"poker_hands = pd.read_csv(\"poker_hand.csv\")\n\n# Define the range of rows to select: row_nums\nrow_nums = range(0, 1000)\n\n# Select the rows using .loc[] and row_nums and record the time before and after\nloc_start_time = time.time()\nrows = poker_hands.loc[row_nums]\nloc_end_time = time.time()\n\n# Print the time it took to select the rows using .loc\nprint(\"Time using .loc[]: {} sec\".format(loc_end_time - loc_start_time))\n\n# Select the rows using .iloc[] and row_nums and record the time before and after\niloc_start_time = time.time()\nrows = poker_hands.iloc[row_nums]\niloc_end_time = time.time()\n\n# Print the time it took to select the rows using .iloc\nprint(\"Time using .iloc[]: {} sec\".format(iloc_end_time - iloc_start_time))","metadata":{"executionTime":61,"lastSuccessfullyExecutedCode":"poker_hands = pd.read_csv(\"poker_hand.csv\")\n\n# Define the range of rows to select: row_nums\nrow_nums = range(0, 1000)\n\n# Select the rows using .loc[] and row_nums and record the time before and after\nloc_start_time = time.time()\nrows = poker_hands.loc[row_nums]\nloc_end_time = time.time()\n\n# Print the time it took to select the rows using .loc\nprint(\"Time using .loc[]: {} sec\".format(loc_end_time - loc_start_time))\n\n# Select the rows using .iloc[] and row_nums and record the time before and after\niloc_start_time = time.time()\nrows = poker_hands.iloc[row_nums]\niloc_end_time = time.time()\n\n# Print the time it took to select the rows using .iloc\nprint(\"Time using .iloc[]: {} sec\".format(iloc_end_time - iloc_start_time))"},"cell_type":"code","id":"9bd2eb4b-5737-4852-84ec-f3c986850556","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .loc[]: 0.0009808540344238281 sec\nTime using .iloc[]: 0.0007779598236083984 sec\n"}]},{"source":"**There is an explanation for that, .iloc() takes advantages of the sorted position of each rows, simplifying the computations needed.**","metadata":{},"cell_type":"markdown","id":"2003471d-f287-4a45-a4cd-bc3fa13f83df"},{"source":"## Column selection: .iloc[] vs by name\nIn the previous exercise, you saw how the .loc[] and .iloc[] functions can be used to locate specific rows of a DataFrame (based on the index). Turns out, the .iloc[] function performs a lot faster (~ 2 times) for this task!\n\nAnother important task is to find the faster function to select the targeted features (columns) of a DataFrame. In this exercise, we will compare the following:\n\n- using the index locator .iloc()\n- using the names of the columns While we can use both functions to perform the same task, we are interested in which is the most efficient in terms of speed.\nIn this exercise, you will continue working with the poker data which is stored in poker_hands. Take a second to examine the structure of this DataFrame by calling poker_hands.head() in the console!","metadata":{},"cell_type":"markdown","id":"6f6941e8-8b87-430a-8b21-34d122140b12"},{"source":"import pandas as pd\nimport time\npoker_hands = pd.read_csv(\"poker_hand.csv\")\n# Use .iloc to select the first, fourth, fifth, seventh and eighth column and record the times before and after\niloc_start_time = time.time()\ncols = poker_hands.iloc[:,[0,3,4,6,7]]\niloc_end_time = time.time()\n\n# Print the time it took\nprint(\"Time using .iloc[] : {} sec\".format(iloc_end_time - iloc_start_time))\n\n# Use simple column selection to select the first, fourth, fifth, seventh and eighth column and record the times before and after\nnames_start_time = time.time()\ncols = poker_hands[['S1', 'S2', 'R2', 'R3', 'S4']]\nnames_end_time = time.time()\n\n# Print the time it took\nprint(\"Time using selection by name : {} sec\".format(names_end_time - names_start_time))","metadata":{"executionTime":20,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport time\npoker_hands = pd.read_csv(\"poker_hand.csv\")\n# Use .iloc to select the first, fourth, fifth, seventh and eighth column and record the times before and after\niloc_start_time = time.time()\ncols = poker_hands.iloc[:,[0,3,4,6,7]]\niloc_end_time = time.time()\n\n# Print the time it took\nprint(\"Time using .iloc[] : {} sec\".format(iloc_end_time - iloc_start_time))\n\n# Use simple column selection to select the first, fourth, fifth, seventh and eighth column and record the times before and after\nnames_start_time = time.time()\ncols = poker_hands[['S1', 'S2', 'R2', 'R3', 'S4']]\nnames_end_time = time.time()\n\n# Print the time it took\nprint(\"Time using selection by name : {} sec\".format(names_end_time - names_start_time))"},"cell_type":"code","id":"6fed9caa-3efe-4b41-8e16-db87d111daa7","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .iloc[] : 0.0006785392761230469 sec\nTime using selection by name : 0.0008842945098876953 sec\n"}]},{"source":"**There is an explanation for that: with .iloc we need to specify both the rows and columns needed, and this takes more time! It might seem insignificant here, but it will make a huge difference in bigger datasets!**","metadata":{},"cell_type":"markdown","id":"8c08b4ec-efdd-4227-9fd2-eb379d48422e"},{"source":"### Random row selection\nIn this exercise, you will compare the two methods described for selecting random rows (entries) with replacement in a pandas DataFrame:\n\n- The built-in pandas function .random()\n- The NumPy random integer number generator np.random.randint()\nGenerally, in the fields of statistics and machine learning, when we need to train an algorithm, we train the algorithm on the 75% of the available data and then test the performance on the remaining 25% of the data.\n\nFor this exercise, we will randomly sample the 75% percent of all the played poker hands available, using each of the above methods, and check which method is more efficient in terms of speed.","metadata":{},"cell_type":"markdown","id":"414170f6-4b12-48cc-85c7-ee5f2f02d001"},{"source":"## Random column selection\nIn the previous exercise, we examined two ways to select random rows from a pandas DataFrame. We can use the same functions to randomly select columns in a pandas DataFrame.\n\nTo randomly select 4 columns out of the poker dataset, you will use the following two functions:\n\n- The built-in pandas function .sample()\n- The NumPy random integer number generator np.random.randint()","metadata":{},"cell_type":"markdown","id":"06c23ddc-385e-4d9c-bc2a-9904dde66f63"},{"source":"import pandas as pd\nimport time\nimport numpy as np\npoker_hands = pd.read_csv(\"poker_hand.csv\")\n# Extract number of rows in dataset\nN=poker_hands.shape[0]\n\n# Select and time the selection of the 75% of the dataset's rows\nrand_start_time = time.time()\npoker_hands.iloc[np.random.randint(low=0, high=N, size=int(0.75 * N))]\nprint(\"Time using Numpy: {} sec\".format(time.time() - rand_start_time))\n\n# Select and time the selection of the 75% of the dataset's rows using sample()\nsamp_start_time = time.time()\npoker_hands.sample(int(0.75 * N), axis=0, replace = True)\nprint(\"Time using .sample: {} sec\".format(time.time() - samp_start_time))","metadata":{"executionTime":34,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport time\nimport numpy as np\npoker_hands = pd.read_csv(\"poker_hand.csv\")\n# Extract number of rows in dataset\nN=poker_hands.shape[0]\n\n# Select and time the selection of the 75% of the dataset's rows\nrand_start_time = time.time()\npoker_hands.iloc[np.random.randint(low=0, high=N, size=int(0.75 * N))]\nprint(\"Time using Numpy: {} sec\".format(time.time() - rand_start_time))\n\n# Select and time the selection of the 75% of the dataset's rows using sample()\nsamp_start_time = time.time()\npoker_hands.sample(int(0.75 * N), axis=0, replace = True)\nprint(\"Time using .sample: {} sec\".format(time.time() - samp_start_time))"},"cell_type":"code","id":"95e0de3a-3ddb-4ae1-b5d7-b80e43365a0a","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using Numpy: 0.0013213157653808594 sec\nTime using .sample: 0.0009236335754394531 sec\n"}]},{"source":"**You found the most efficient way to sample random rows from a pandas DataFrame, and it's always the built-in function.**","metadata":{},"cell_type":"markdown","id":"bdca7d01-d035-497b-a8c7-968554616076"},{"source":"import pandas as pd\nimport numpy as np\nimport time\npoker_hands = pd.read_csv(\"poker_hand.csv\")","metadata":{"executionTime":58,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np\nimport time\npoker_hands = pd.read_csv(\"poker_hand.csv\")"},"cell_type":"code","id":"3118d978-1dc4-4941-bee0-ac9761cb8da1","execution_count":1,"outputs":[]},{"source":"### Replacing scalar values I\nIn this exercise, we will replace a list of values in our dataset by using the .replace() method with another list of desired values.\n\nWe will apply the functions in the poker_hands DataFrame. Remember that in the poker_hands DataFrame, each row of columns R1 to R5 represents the rank of each card from a player's poker hand spanning from 1 (Ace) to 13 (King). The Class feature classifies each hand as a category, and the Explanation feature briefly explains each hand.\n\nThe poker_hands DataFrame is already loaded for you, and you can explore the features Class and Explanation.\n\nRemember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab.","metadata":{},"cell_type":"markdown","id":"b01bd87b-ba4f-49ac-bf13-d326fa9cfbc7"},{"source":"poker_hands.head()","metadata":{"executionTime":135,"lastSuccessfullyExecutedCode":"poker_hands.head()"},"cell_type":"code","id":"63a78808-af50-4c62-a1c0-2c21ae4ca4c8","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"S1","type":"integer"},{"name":"R1","type":"integer"},{"name":"S2","type":"integer"},{"name":"R2","type":"integer"},{"name":"S3","type":"integer"},{"name":"R3","type":"integer"},{"name":"S4","type":"integer"},{"name":"R4","type":"integer"},{"name":"S5","type":"integer"},{"name":"R5","type":"integer"},{"name":"Class","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"S1":1,"R1":10,"S2":1,"R2":11,"S3":1,"R3":13,"S4":1,"R4":12,"S5":1,"R5":1,"Class":9},{"index":1,"S1":2,"R1":11,"S2":2,"R2":13,"S3":2,"R3":10,"S4":2,"R4":12,"S5":2,"R5":1,"Class":9},{"index":2,"S1":3,"R1":12,"S2":3,"R2":11,"S3":3,"R3":13,"S4":3,"R4":10,"S5":3,"R5":1,"Class":9},{"index":3,"S1":4,"R1":10,"S2":4,"R2":11,"S3":4,"R3":1,"S4":4,"R4":13,"S5":4,"R5":12,"Class":9},{"index":4,"S1":4,"R1":1,"S2":4,"R2":13,"S3":4,"R3":12,"S4":4,"R4":11,"S5":4,"R5":10,"Class":9}]},"total_rows":5,"truncation_type":null},"text/plain":"   S1  R1  S2  R2  S3  R3  S4  R4  S5  R5  Class\n0   1  10   1  11   1  13   1  12   1   1      9\n1   2  11   2  13   2  10   2  12   2   1      9\n2   3  12   3  11   3  13   3  10   3   1      9\n3   4  10   4  11   4   1   4  13   4  12      9\n4   4   1   4  13   4  12   4  11   4  10      9","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>S1</th>\n      <th>R1</th>\n      <th>S2</th>\n      <th>R2</th>\n      <th>S3</th>\n      <th>R3</th>\n      <th>S4</th>\n      <th>R4</th>\n      <th>S5</th>\n      <th>R5</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11</td>\n      <td>2</td>\n      <td>13</td>\n      <td>2</td>\n      <td>10</td>\n      <td>2</td>\n      <td>12</td>\n      <td>2</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>12</td>\n      <td>3</td>\n      <td>11</td>\n      <td>3</td>\n      <td>13</td>\n      <td>3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>10</td>\n      <td>4</td>\n      <td>11</td>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>13</td>\n      <td>4</td>\n      <td>12</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>13</td>\n      <td>4</td>\n      <td>12</td>\n      <td>4</td>\n      <td>11</td>\n      <td>4</td>\n      <td>10</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# Replace Class 1 to -2 \npoker_hands['Class'].replace(1, -2, inplace=True)\n# Replace Class 2 to -3\npoker_hands['Class'].replace(2, -3, inplace=True)\n\nprint(poker_hands[['Class']])","metadata":{"executionTime":67,"lastSuccessfullyExecutedCode":"# Replace Class 1 to -2 \npoker_hands['Class'].replace(1, -2, inplace=True)\n# Replace Class 2 to -3\npoker_hands['Class'].replace(2, -3, inplace=True)\n\nprint(poker_hands[['Class']])"},"cell_type":"code","id":"a6588b1c-eb8b-4bb4-b2e7-23f8aeeac529","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"       Class\n0          9\n1          9\n2          9\n3          9\n4          9\n...      ...\n25005      0\n25006     -2\n25007     -2\n25008     -2\n25009     -2\n\n[25010 rows x 1 columns]\n"}]},{"source":"### Replace scalar values II\nAs discussed in the video, in a pandas DataFrame, it is possible to replace values in a very intuitive way: we locate the position (row and column) in the Dataframe and assign in the new value you want to replace with. In a more pandas-ian way, the .replace() function is available that performs the same task.\n\nYou will be using the names DataFrame which includes, among others, the most popular names in the US by year, gender and ethnicity.\n\nYour task is to replace all the babies that are classified as FEMALE to GIRL using the following methods:\n\n- intuitive scalar replacement\n- using the .replace() function","metadata":{},"cell_type":"markdown","id":"be7d1431-7663-4c61-ac2a-2a595399e51b"},{"source":"names = pd.read_csv(\"Popular_Baby_Names.csv\")","metadata":{"executionTime":32,"lastSuccessfullyExecutedCode":"names = pd.read_csv(\"Popular_Baby_Names.csv\")"},"cell_type":"code","id":"d012a4da-4eb2-4a45-96f2-d19181503b2e","execution_count":6,"outputs":[]},{"source":"start_time = time.time()\n\n# Replace all the entries that has 'FEMALE' as a gender with 'GIRL'\nnames['Gender'].loc[names.Gender == 'FEMALE'] = 'GIRL'\n\nprint(\"Time using .loc[]: {} sec\".format(time.time() - start_time))","metadata":{"executionTime":21,"lastSuccessfullyExecutedCode":"start_time = time.time()\n\n# Replace all the entries that has 'FEMALE' as a gender with 'GIRL'\nnames['Gender'].loc[names.Gender == 'FEMALE'] = 'GIRL'\n\nprint(\"Time using .loc[]: {} sec\".format(time.time() - start_time))"},"cell_type":"code","id":"6f98610f-a002-4457-96a9-4801639c53c1","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .loc[]: 0.0024924278259277344 sec\n"}]},{"source":"start_time = time.time()\n\n# Replace all the entries that has 'FEMALE' as a gender with 'GIRL'\nnames['Gender'].replace('FEMALE', 'GIRL', inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))","metadata":{"executionTime":20,"lastSuccessfullyExecutedCode":"start_time = time.time()\n\n# Replace all the entries that has 'FEMALE' as a gender with 'GIRL'\nnames['Gender'].replace('FEMALE', 'GIRL', inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))"},"cell_type":"code","id":"99808498-1511-4bf1-82cd-1f4a5b74c8d4","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .replace(): 0.0008122920989990234 sec\n"}]},{"source":"**You found the most efficient way to replace a scalar value on a pandas DataFrame! Exciting, isn't it?**","metadata":{},"cell_type":"markdown","id":"684e7838-dc87-40fd-851a-33f92b853275"},{"source":"### Replace multiple values I\nIn this exercise, you will apply the .replace() function for the task of replacing multiple values with one or more values. You will again use the names dataset which contains, among others, the most popular names in the US by year, gender and Ethnicity.\n\nThus you want to replace all ethnicities classified as black or white non-hispanics to non-hispanic. Remember, the ethnicities are stated in the dataset as follows: ['BLACK NON HISP', 'BLACK NON HISPANIC', 'WHITE NON HISP' , 'WHITE NON HISPANIC'] and should be replaced to 'NON HISPANIC'","metadata":{},"cell_type":"markdown","id":"dbe99bfe-02ca-4257-8db8-207f4f73ba87"},{"source":"start_time = time.time()\n\n# Replace all non-Hispanic ethnicities with 'NON HISPANIC'\nnames['Ethnicity'].loc[(names[\"Ethnicity\"] == 'BLACK NON HISP') | \n                      (names[\"Ethnicity\"] == 'BLACK NON HISPANIC') | \n                      (names[\"Ethnicity\"] == 'WHITE NON HISP') | \n                      (names[\"Ethnicity\"] == 'WHITE NON HISPANIC')] = 'NON HISPANIC'\n\nprint(\"Time using .loc[]: sec\".format(time.time() - start_time))","metadata":{"executionTime":20,"lastSuccessfullyExecutedCode":"start_time = time.time()\n\n# Replace all non-Hispanic ethnicities with 'NON HISPANIC'\nnames['Ethnicity'].loc[(names[\"Ethnicity\"] == 'BLACK NON HISP') | \n                      (names[\"Ethnicity\"] == 'BLACK NON HISPANIC') | \n                      (names[\"Ethnicity\"] == 'WHITE NON HISP') | \n                      (names[\"Ethnicity\"] == 'WHITE NON HISPANIC')] = 'NON HISPANIC'\n\nprint(\"Time using .loc[]: sec\".format(time.time() - start_time))"},"cell_type":"code","id":"dfad6874-7f41-4a64-8a6e-83f941305e92","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .loc[]: sec\n"}]},{"source":"start_time = time.time()\n\n# Replace all non-Hispanic ethnicities with 'NON HISPANIC'\nnames['Ethnicity'].replace(['BLACK NON HISP', 'BLACK NON HISPANIC', 'WHITE NON HISP', 'WHITE NON HISPANIC'], 'NON HISPANIC', inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))","metadata":{"executionTime":28,"lastSuccessfullyExecutedCode":"start_time = time.time()\n\n# Replace all non-Hispanic ethnicities with 'NON HISPANIC'\nnames['Ethnicity'].replace(['BLACK NON HISP', 'BLACK NON HISPANIC', 'WHITE NON HISP', 'WHITE NON HISPANIC'], 'NON HISPANIC', inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))"},"cell_type":"code","id":"ade02f9f-f566-48bf-b2ec-b1c4b14e9aca","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .replace(): 0.003183603286743164 sec\n"}]},{"source":"### Replace multiple values II\nAs discussed in the video, instead of using the .replace() function multiple times to replace multiple values, you can use lists to map the elements you want to replace one to one with those you want to replace them with.\n\nAs you have seen in our popular names dataset, there are two names for the same ethnicity. We want to standardize the naming of each ethnicity by replacing\n\n- 'ASIAN AND PACI' to 'ASIAN AND PACIFIC ISLANDER'\n- 'BLACK NON HISP' to 'BLACK NON HISPANIC'\n- 'WHITE NON HISP' to 'WHITE NON HISPANIC'\nIn the DataFrame names, you are going to replace all the values on the left by the values on the right.","metadata":{},"cell_type":"markdown","id":"a2605a31-7f95-4ed8-a46a-f5b156cb804a"},{"source":"import pandas as pd\nimport time\nnames = pd.read_csv(\"Popular_Baby_Names.csv\")\nstart_time = time.time()\n\n# Replace ethnicities as instructed\nnames['Ethnicity'].replace(['ASIAN AND PACI','BLACK NON HISP', 'WHITE NON HISP'], ['ASIAN AND PACIFIC ISLANDER','BLACK NON HISPANIC','WHITE NON HISPANIC'], inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))","metadata":{"executionTime":51,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport time\nnames = pd.read_csv(\"Popular_Baby_Names.csv\")\nstart_time = time.time()\n\n# Replace ethnicities as instructed\nnames['Ethnicity'].replace(['ASIAN AND PACI','BLACK NON HISP', 'WHITE NON HISP'], ['ASIAN AND PACIFIC ISLANDER','BLACK NON HISPANIC','WHITE NON HISPANIC'], inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))"},"cell_type":"code","id":"d822bbc3-16b7-42ab-900e-6373d61ff830","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using .replace(): 0.002157449722290039 sec\n"}]},{"source":"### Replace single values I\nIn this exercise, we will apply the following replacing technique of replacing multiple values using dictionaries on a different dataset.\n\nWe will apply the functions in the data DataFrame. Each row represents the rank of 5 cards from a playing card deck, spanning from 1 (Ace) to 13 (King) (features R1, R2, R3, R4, R5). The feature 'Class' classifies each row to a category (from 0 to 9) and the feature 'Explanation' gives a brief explanation of what each class represents.\n\nThe purpose of this exercise is to categorize the two types of flush in the game ('Royal flush' and 'Straight flush') under the 'Flush' name.","metadata":{},"cell_type":"markdown","id":"851421fb-4378-4831-8dbd-2c36d9fe6f10"},{"source":"poker_hands = pd.read_csv(\"poker_hand.csv\")\n# Replace Royal flush or Straight flush to Flush\npoker_hands.replace({'Royal flush':'Flush', 'Straight flush':'Flush'}, inplace=True)\nprint(poker_hands['Explanation'].head())","metadata":{"executionTime":1980,"lastSuccessfullyExecutedCode":"poker_hands = pd.read_csv(\"poker_hand.csv\")\n# Replace Royal flush or Straight flush to Flush\npoker_hands.replace({'Royal flush':'Flush', 'Straight flush':'Flush'}, inplace=True)\nprint(poker_hands['Explanation'].head())","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"70bb5f18-70da-4c13-9985-79cdad71e51c","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Explanation'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Replace Royal flush or Straight flush to Flush\u001b[39;00m\n\u001b[1;32m      3\u001b[0m poker_hands\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoyal flush\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlush\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStraight flush\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlush\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpoker_hands\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExplanation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3804\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3806\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'Explanation'"],"ename":"KeyError","evalue":"'Explanation'"}]},{"source":"### Replace single values II\nFor this exercise, we will be using the names DataFrame. In this dataset, the column 'Rank' shows the ranking of each name by year. For this exercise, you will use dictionaries to replace the first ranked name of every year as 'FIRST', the second name as 'SECOND' and the third name as 'THIRD'.\n\nYou will use dictionaries to replace one single value per key.\n\nYou can already see the first 5 names of the data, which correspond to the 5 most popular names for all the females belonging to the 'ASIAN AND PACIFIC ISLANDER' ethnicity in 2011.","metadata":{},"cell_type":"markdown","id":"0b49246e-c19e-43f5-91a8-273b7651ea78"},{"source":"# Replace the number rank by a string\nnames['Rank'].replace({1:'FIRST', 2:'SECOND', 3:'THIRD'}, inplace=True)\nprint(names.head())","metadata":{"executionTime":29,"lastSuccessfullyExecutedCode":"# Replace the number rank by a string\nnames['Rank'].replace({1:'FIRST', 2:'SECOND', 3:'THIRD'}, inplace=True)\nprint(names.head())"},"cell_type":"code","id":"371e9c1e-926f-4012-b789-78812dbb987c","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"   Year of Birth  Gender  ... Count    Rank\n0           2011  FEMALE  ...   119   FIRST\n1           2011  FEMALE  ...   106  SECOND\n2           2011  FEMALE  ...    93   THIRD\n3           2011  FEMALE  ...    89       4\n4           2011  FEMALE  ...    75       5\n\n[5 rows x 6 columns]\n"}]},{"source":"### Replace multiple values III\nAs you saw in the video, you can use dictionaries to replace multiple values with just one value, even from multiple columns. To show the usefulness of replacing with dictionaries, you will use the names dataset one more time.\n\nIn this dataset, the column 'Rank' shows which rank each name reached every year. You will change the rank of the first three ranked names of every year to 'MEDAL' and those from 4th and 5th place to 'ALMOST MEDAL'.\n\nYou can already see the first 5 names of the data, which correspond to the 5 most popular names for all the females belonging to the 'ASIAN AND PACIFIC ISLANDER' ethnicity in 2011.","metadata":{},"cell_type":"markdown","id":"0fcd1b6b-bd2c-4cf9-b5a5-4bb0d404122b"},{"source":"# Replace the rank of the first three ranked names to 'MEDAL'\nnames.replace({'Rank': {1:'MEDAL', 2:'MEDAL', 3:'MEDAL'}}, inplace=True)\n\n# Replace the rank of the 4th and 5th ranked names to 'ALMOST MEDAL'\nnames.replace({'Rank': {4:'ALMOST MEDAL', 5:'ALMOST MEDAL'}}, inplace=True)\nprint(names.head())","metadata":{"executionTime":25,"lastSuccessfullyExecutedCode":"# Replace the rank of the first three ranked names to 'MEDAL'\nnames.replace({'Rank': {1:'MEDAL', 2:'MEDAL', 3:'MEDAL'}}, inplace=True)\n\n# Replace the rank of the 4th and 5th ranked names to 'ALMOST MEDAL'\nnames.replace({'Rank': {4:'ALMOST MEDAL', 5:'ALMOST MEDAL'}}, inplace=True)\nprint(names.head())"},"cell_type":"code","id":"8456962e-f4ce-4c09-b0f3-dc0cc0415dc2","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"   Year of Birth  Gender  ... Count          Rank\n0           2011  FEMALE  ...   119         FIRST\n1           2011  FEMALE  ...   106        SECOND\n2           2011  FEMALE  ...    93         THIRD\n3           2011  FEMALE  ...    89  ALMOST MEDAL\n4           2011  FEMALE  ...    75  ALMOST MEDAL\n\n[5 rows x 6 columns]\n"}]},{"source":"### Create a generator for a pandas DataFrame\nAs you've seen in the video, you can easily create a generator out of a pandas DataFrame. Each time you iterate through it, it will yield two elements:\n\n- the index of the respective row\n- a pandas Series with all the elements of that row\nYou are going to create a generator over the poker dataset, imported as poker_hands. Then, you will print all the elements of the 2nd row, using the generator.\n\nRemember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab.","metadata":{},"cell_type":"markdown","id":"ec80ca73-693a-407c-9788-71b90d0e94fa"},{"source":"import pandas as pd\nimport time\npoker_hands = pd.read_csv(\"poker_hand.csv\")\n\n# Create a generator over the rows\ngenerator = poker_hands.iterrows()\n\n# Access the elements of the 2nd row\nfirst_element = next(generator)\nsecond_element = next(generator)\nprint(first_element, second_element)","metadata":{"executionTime":53,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport time\npoker_hands = pd.read_csv(\"poker_hand.csv\")\n\n# Create a generator over the rows\ngenerator = poker_hands.iterrows()\n\n# Access the elements of the 2nd row\nfirst_element = next(generator)\nsecond_element = next(generator)\nprint(first_element, second_element)"},"cell_type":"code","id":"be92c299-2aae-484f-8a4a-3ce590d03646","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"(0, S1        1\nR1       10\nS2        1\nR2       11\nS3        1\nR3       13\nS4        1\nR4       12\nS5        1\nR5        1\nClass     9\nName: 0, dtype: int64) (1, S1        2\nR1       11\nS2        2\nR2       13\nS3        2\nR3       10\nS4        2\nR4       12\nS5        2\nR5        1\nClass     9\nName: 1, dtype: int64)\n"}]},{"source":"### The iterrows() function for looping\nYou just saw how to create a generator out of a pandas DataFrame. You will now use this generator and see how to take advantage of that method of looping through a pandas DataFrame, still using the poker_hands dataset.\n\nSpecifically, we want the sum of the ranks of all the cards, if the index of the hand is an odd number. The ranks of the cards are located in the odd columns of the DataFrame.","metadata":{},"cell_type":"markdown","id":"ac61a2ff-f241-4a07-9a3a-c827a0df7b51"},{"source":"data_generator = poker_hands.iterrows()\n\nfor index, values in data_generator:\n  \t# Check if index is odd\n    if index % 2 !=0:\n      \t# Sum the ranks of all the cards\n        hand_sum = sum([values[1], values[3], values[5], values[7], values[9]])","metadata":{"executionTime":751,"lastSuccessfullyExecutedCode":"data_generator = poker_hands.iterrows()\n\nfor index, values in data_generator:\n  \t# Check if index is odd\n    if index % 2 !=0:\n      \t# Sum the ranks of all the cards\n        hand_sum = sum([values[1], values[3], values[5], values[7], values[9]])"},"cell_type":"code","id":"a41c9431-9f95-463a-97fe-6f89db4361b7","execution_count":3,"outputs":[]},{"source":"### .apply() function in every cell\nAs you saw in the lesson, you can use .apply() to map a function to every cell of the DataFrame, regardless the column or the row.\n\nYou're going to try it out on the poker_hands dataset. You will use .apply() to square every cell of the DataFrame. The native Python way to square a number n is n**2.","metadata":{},"cell_type":"markdown","id":"7192c2fb-79ce-4423-8671-2b717a7362f8"},{"source":"# Define the lambda transformation\nget_square = lambda x: x**2\n\n# Apply the transformation\ndata_sum = poker_hands.apply(get_square)\nprint(data_sum.head())","metadata":{"executionTime":35,"lastSuccessfullyExecutedCode":"# Define the lambda transformation\nget_square = lambda x: x**2\n\n# Apply the transformation\ndata_sum = poker_hands.apply(get_square)\nprint(data_sum.head())"},"cell_type":"code","id":"a1bd0470-d5e9-45e5-a30a-6f7e2bc0671f","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"   S1   R1  S2   R2  S3   R3  S4   R4  S5   R5  Class\n0   1  100   1  121   1  169   1  144   1    1     81\n1   4  121   4  169   4  100   4  144   4    1     81\n2   9  144   9  121   9  169   9  100   9    1     81\n3  16  100  16  121  16    1  16  169  16  144     81\n4  16    1  16  169  16  144  16  121  16  100     81\n"}]},{"source":"### .apply() for rows iteration\n.apply() is a very useful to iterate through the rows of a DataFrame and apply a specific function.\n\nYou will work on a subset of the poker_hands dataset, which includes only the rank of all the five cards of each hand in each row (this subset is generated for you in the script). You're going to get the variance of every hand for all ranks, and every rank for all hands.","metadata":{},"cell_type":"markdown","id":"7fc5ce03-6e7f-45cf-9274-c87f2a13d22c"},{"source":"import numpy as np\nget_variance = lambda x: np.var(x)\n\n# Apply the transformation\ndata_tr = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].apply(get_variance, axis=0)\nprint(data_tr.head())","metadata":{"executionTime":29,"lastSuccessfullyExecutedCode":"import numpy as np\nget_variance = lambda x: np.var(x)\n\n# Apply the transformation\ndata_tr = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].apply(get_variance, axis=0)\nprint(data_tr.head())"},"cell_type":"code","id":"02431fc0-71a0-401f-b554-e7dce4b5db40","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"R1    14.060473\nR2    14.189523\nR3    14.024270\nR4    14.040552\nR5    13.998851\ndtype: float64\n"}]},{"source":"**Vectorization is always the fastest option, and now you know why. Extra tip: It uses C code the optimal way!**","metadata":{},"cell_type":"markdown","id":"77ac3303-85fa-460b-a4e7-aae123aa1b7e"},{"source":"### pandas vectorization in action\nIn this exercise, you will apply vectorization over pandas series to:\n\n- calculate the mean rank of all the cards in each hand (row)\n- calculate the mean rank of each of the 5 cards in each hand (column)\nYou will use the poker_hands dataset once again to compare both methods' efficiency.","metadata":{},"cell_type":"markdown","id":"6eddd6a1-f684-4cd5-aa78-0f2f84470bbc"},{"source":"# Calculate the mean rank in each hand\nrow_start_time = time.time()\nmean_r = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].mean(axis=1)\nprint(\"Time using pandas vectorization for rows: {} sec\".format(time.time() - row_start_time))\nprint(mean_r.head())\n\n# Calculate the mean rank of each of the 5 card in all hands\ncol_start_time = time.time()\nmean_c = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].mean(axis=0)\nprint(\"Time using pandas vectorization for columns: {} sec\".format(time.time() - col_start_time))\nprint(mean_c.head())","metadata":{"executionTime":23,"lastSuccessfullyExecutedCode":"# Calculate the mean rank in each hand\nrow_start_time = time.time()\nmean_r = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].mean(axis=1)\nprint(\"Time using pandas vectorization for rows: {} sec\".format(time.time() - row_start_time))\nprint(mean_r.head())\n\n# Calculate the mean rank of each of the 5 card in all hands\ncol_start_time = time.time()\nmean_c = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].mean(axis=0)\nprint(\"Time using pandas vectorization for columns: {} sec\".format(time.time() - col_start_time))\nprint(mean_c.head())"},"cell_type":"code","id":"0dc2ac3d-f705-4d01-aab3-15dcdfb4e516","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using pandas vectorization for rows: 0.002193450927734375 sec\n0    9.4\n1    9.4\n2    9.4\n3    9.4\n4    9.4\ndtype: float64\nTime using pandas vectorization for columns: 0.0014700889587402344 sec\nR1    6.995242\nR2    7.014194\nR3    7.014154\nR4    6.942463\nR5    6.962735\ndtype: float64\n"}]},{"source":"### Vectorization methods for looping a DataFrame\nNow that you're familiar with vectorization in pandas and NumPy, you're going to compare their respective performances yourself.\n\nYour task is to calculate the variance of all the hands in each hand using the vectorization over pandas Series and then modify your code using the vectorization over Numpy ndarrays method.","metadata":{},"cell_type":"markdown","id":"f89a16b6-6410-4c7a-af12-88f44c9097b0"},{"source":"# Calculate the variance in each hand\nstart_time = time.time()\npoker_var = poker_hands[[\"R1\", \"R2\", \"R3\", \"R4\", \"R5\"]].var(axis=1)\nprint(\"Time using pandas vectorization: {} sec\".format(time.time() - start_time))\nprint(poker_var.head())","metadata":{"executionTime":25,"lastSuccessfullyExecutedCode":"# Calculate the variance in each hand\nstart_time = time.time()\npoker_var = poker_hands[[\"R1\", \"R2\", \"R3\", \"R4\", \"R5\"]].var(axis=1)\nprint(\"Time using pandas vectorization: {} sec\".format(time.time() - start_time))\nprint(poker_var.head())"},"cell_type":"code","id":"715d417f-62e9-410a-9f42-3ea749e3eec9","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using pandas vectorization: 0.0035152435302734375 sec\n0    23.3\n1    23.3\n2    23.3\n3    23.3\n4    23.3\ndtype: float64\n"}]},{"source":"# Calculate the variance in each hand\nstart_time = time.time()\npoker_var = poker_hands[['R1','R2','R3','R4','R5']].values.var(axis=1, ddof=1)\nprint(\"Time using NumPy vectorization: {} sec\".format(time.time() - start_time))\nprint(poker_var[0:5])","metadata":{"executionTime":22,"lastSuccessfullyExecutedCode":"# Calculate the variance in each hand\nstart_time = time.time()\npoker_var = poker_hands[['R1','R2','R3','R4','R5']].values.var(axis=1, ddof=1)\nprint(\"Time using NumPy vectorization: {} sec\".format(time.time() - start_time))\nprint(poker_var[0:5])"},"cell_type":"code","id":"c7084dec-132f-4c15-8fb1-d1937147e5bb","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"Time using NumPy vectorization: 0.0025031566619873047 sec\n[23.3 23.3 23.3 23.3 23.3]\n"}]},{"source":"### The min-max normalization using .transform()\nA very common operation is the min-max normalization. It consists in rescaling our value of interest by deducting the minimum value and dividing the result by the difference between the maximum and the minimum value. For example, to rescale student's weight data spanning from 160 pounds to 200 pounds, you subtract 160 from each student's weight and divide the result by 40 (200 - 160).\n\nYou're going to define and apply the min-max normalization to all the numerical variables in the restaurant data. You will first group the entries by the time the meal took place (Lunch or Dinner) and then apply the normalization to each group separately.\n\nRemember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab.","metadata":{},"cell_type":"markdown","id":"f0e7661a-d95f-44c9-87b8-1ca18fb3635a"},{"source":"import pandas as pd\nrestaurant_data = pd.read_csv(\"restaurant_data.csv\")\n# Define the min-max transformation\nmin_max_tr = lambda x: (x - x.min()) / (x.max() - x.min())\n\n# Group the data according to the time\nrestaurant_grouped = restaurant_data.groupby(\"time\")\n\n# Apply the transformation\nrestaurant_min_max_group = restaurant_grouped.transform(min_max_tr)\nprint(restaurant_min_max_group.head())","metadata":{"executionTime":43,"lastSuccessfullyExecutedCode":"import pandas as pd\nrestaurant_data = pd.read_csv(\"restaurant_data.csv\")\n# Define the min-max transformation\nmin_max_tr = lambda x: (x - x.min()) / (x.max() - x.min())\n\n# Group the data according to the time\nrestaurant_grouped = restaurant_data.groupby(\"time\")\n\n# Apply the transformation\nrestaurant_min_max_group = restaurant_grouped.transform(min_max_tr)\nprint(restaurant_min_max_group.head())"},"cell_type":"code","id":"bc6f2255-5eea-4f9e-98fd-a9fe75ec3173","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"   total_bill       tip  size\n0    0.291579  0.001111   0.2\n1    0.152283  0.073333   0.4\n2    0.375786  0.277778   0.4\n3    0.431713  0.256667   0.2\n4    0.450775  0.290000   0.6\n"}]},{"source":"### Transforming values to probabilities\nIn this exercise, we will apply a probability distribution function to a pandas DataFrame with group related parameters by transforming the tip variable to probabilities.\n\nThe transformation will be a exponential transformation. The exponential distribution is defined as\n\n\nwhere λ (lambda) is the mean of the group that the observation x belongs to.\n\nYou're going to apply the exponential distribution transformation to the size of each table in the dataset, after grouping the data according to the time of the day the meal took place. Remember to use each group's mean for the value of λ.\n\nIn Python, you can use the exponential as np.exp() from the NumPy library and the mean value as .mean().","metadata":{},"cell_type":"markdown","id":"1630abfb-8312-47ca-a922-9c8b00eb9bd8"},{"source":"import numpy as np\n# Define the exponential transformation\nexp_tr = lambda x: np.exp(x.mean()*-x) * x.mean()\n\n# Group the data according to the time\nrestaurant_grouped = restaurant_data.groupby(\"time\")\n\n# Apply the transformation\nrestaurant_exp_group = restaurant_grouped['tip'].transform(exp_tr)\nprint(restaurant_exp_group.head())","metadata":{"executionTime":25,"lastSuccessfullyExecutedCode":"import numpy as np\n# Define the exponential transformation\nexp_tr = lambda x: np.exp(x.mean()*-x) * x.mean()\n\n# Group the data according to the time\nrestaurant_grouped = restaurant_data.groupby(\"time\")\n\n# Apply the transformation\nrestaurant_exp_group = restaurant_grouped['tip'].transform(exp_tr)\nprint(restaurant_exp_group.head())"},"cell_type":"code","id":"b917a32d-20b5-416f-a7ae-28b79de4c6b3","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"0    0.135141\n1    0.017986\n2    0.000060\n3    0.000108\n4    0.000042\nName: tip, dtype: float64\n"}]},{"source":"### Validation of normalization\nFor this exercise, we will perform a z-score normalization and verify that it was performed correctly.\n\nA distinct characteristic of normalized values is that they have a mean equal to zero and standard deviation equal to one.\n\nAfter you apply the normalization transformation, you can group again on the same variable, and then check the mean and the standard deviation of each group.\n\nYou will apply the normalization transformation to every numeric variable in the poker_grouped dataset, which is the poker_hands dataset grouped by Class.","metadata":{},"cell_type":"markdown","id":"3fc0ef14-6f3a-407f-b390-ec2ef42cf14b"},{"source":"poker_hands = pd.read_csv(\"poker_hand.csv\")\nzscore = lambda x: (x - x.mean()) / x.std()\npoker_grouped = poker_hands.groupby(\"Class\")\n# Apply the transformation\npoker_trans = poker_grouped.transform(zscore)\n\n# Re-group the grouped object and print each group's means and standard deviation\npoker_regrouped = poker_trans.groupby(poker_hands[\"Class\"])\n\nprint(np.round(poker_regrouped.mean(), 3))\nprint(poker_regrouped.var())","metadata":{"executionTime":89,"lastSuccessfullyExecutedCode":"poker_hands = pd.read_csv(\"poker_hand.csv\")\nzscore = lambda x: (x - x.mean()) / x.std()\npoker_grouped = poker_hands.groupby(\"Class\")\n# Apply the transformation\npoker_trans = poker_grouped.transform(zscore)\n\n# Re-group the grouped object and print each group's means and standard deviation\npoker_regrouped = poker_trans.groupby(poker_hands[\"Class\"])\n\nprint(np.round(poker_regrouped.mean(), 3))\nprint(poker_regrouped.var())"},"cell_type":"code","id":"fb7f829d-6055-4057-96a3-91c1297e9293","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"        S1   R1   S2   R2   S3   R3   S4   R4   S5   R5\nClass                                                  \n0     -0.0  0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.0 -0.0 -0.0\n1      0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0 -0.0  0.0\n2     -0.0  0.0  0.0 -0.0 -0.0  0.0  0.0 -0.0  0.0  0.0\n3      0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.0  0.0  0.0\n4     -0.0 -0.0 -0.0 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0\n5     -0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0  0.0\n6     -0.0 -0.0 -0.0  0.0  0.0 -0.0  0.0  0.0 -0.0  0.0\n7      0.0 -0.0 -0.0  0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.0\n8     -0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.0\n9      0.0 -0.0  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0\n        S1   R1   S2   R2   S3   R3   S4   R4   S5   R5\nClass                                                  \n0      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n1      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n2      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n3      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n4      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n5      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n6      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n7      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n8      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n9      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n"}]},{"source":"### Identifying missing values\nThe first step before missing value imputation is to identify if there are missing values in our data, and if so, from which group they arise.\n\nFor the same restaurant_data data you encountered in the lesson, an employee erased by mistake the tips left in 65 tables. The question at stake is how many missing entries came from tables that smokers where present vs tables with no-smokers present.\n\nYour task is to group both datasets according to the smoker variable, count the number or present values and then calculate the difference.\n\nWe're imputing tips to get you to practice the concepts taught in the lesson. From an ethical standpoint, you should not impute financial data in real life, as it could be considered fraud.","metadata":{},"cell_type":"markdown","id":"b56e2b76-f438-4f2d-91dc-b0b4a7f5765d"},{"source":"# Group both objects according to smoke condition\nrestaurant_nan_grouped = restaurant_nan.groupby('smoker')\n\n# Store the number of present values\nrestaurant_nan_nval = restaurant_nan_grouped['tip'].count()\n\n# Print the group-wise missing entries\nprint(restaurant_nan_grouped['total_bill'].count() - restaurant_nan_nval)","metadata":{"executionTime":39,"lastSuccessfullyExecutedCode":"# Group both objects according to smoke condition\nrestaurant_nan_grouped = restaurant_nan.groupby('smoker')\n\n# Store the number of present values\nrestaurant_nan_nval = restaurant_nan_grouped['tip'].count()\n\n# Print the group-wise missing entries\nprint(restaurant_nan_grouped['total_bill'].count() - restaurant_nan_nval)","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"d023f4b4-ec12-4273-a4ad-f788182ca2aa","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Group both objects according to smoke condition\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m restaurant_nan_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mrestaurant_nan\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Store the number of present values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m restaurant_nan_nval \u001b[38;5;241m=\u001b[39m restaurant_nan_grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\n","\u001b[0;31mNameError\u001b[0m: name 'restaurant_nan' is not defined"],"ename":"NameError","evalue":"name 'restaurant_nan' is not defined"}]},{"source":"### Missing value imputation\nAs the majority of the real world data contain missing entries, replacing these entries with sensible values can increase the insight you can get from our data.\n\nIn the restaurant dataset, the \"total_bill\" column has some missing entries, meaning that you have not recorded how much some tables have paid. Your task in this exercise is to replace the missing entries with the median value of the amount paid, according to whether the entry was recorded on lunch or dinner (time variable).","metadata":{},"cell_type":"markdown","id":"bab4825e-7e99-4c48-814e-bb3dbf1d53d9"},{"source":"# Define the lambda function\nmissing_trans = lambda x: x.fillna(x.median())\n\n# Group the data according to time\nrestaurant_grouped = restaurant_data.groupby(\"time\")\n\n# Apply the transformation\nrestaurant_impute = restaurant_grouped.transform(missing_trans)\nprint(restaurant_impute.head())","metadata":{},"cell_type":"code","id":"b9f77803-b052-45f4-bffb-9da1975ecb14","execution_count":null,"outputs":[]},{"source":"### Data filtration\nAs you noticed in the video lesson, you may need to filter your data for various reasons.\n\nIn this exercise, you will use filtering to select a specific part of our DataFrame:\n\nby the number of entries recorded in each day of the week\nby the mean amount of money the customers paid to the restaurant each day of the week","metadata":{},"cell_type":"markdown","id":"e26d6570-1385-4ed0-96fc-6f86dcf7e97f"},{"source":"# Filter the days where the count of total_bill is greater than $40\ntotal_bill_40 = restaurant_data.groupby('day').filter(lambda x: x['total_bill'].count() > 40)\n\n# Select only the entries that have a mean total_bill greater than $20\ntotal_bill_20 = total_bill_40.groupby('day').filter(lambda x : x['total_bill'].mean() > 20)\n\n# Print days of the week that have a mean total_bill greater than $20\nprint('Days of the week that have a mean total_bill greater than $20:', total_bill_20.day.unique())","metadata":{"executionTime":24,"lastSuccessfullyExecutedCode":"# Filter the days where the count of total_bill is greater than $40\ntotal_bill_40 = restaurant_data.groupby('day').filter(lambda x: x['total_bill'].count() > 40)\n\n# Select only the entries that have a mean total_bill greater than $20\ntotal_bill_20 = total_bill_40.groupby('day').filter(lambda x : x['total_bill'].mean() > 20)\n\n# Print days of the week that have a mean total_bill greater than $20\nprint('Days of the week that have a mean total_bill greater than $20:', total_bill_20.day.unique())"},"cell_type":"code","id":"78282b24-379c-4fb7-97fc-d36a1ccd9ba7","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"Days of the week that have a mean total_bill greater than $20: ['Sun' 'Sat']\n"}]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}